{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete and Continuous Control with A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) is the study of agents taking actions in an environment in order to increase their \"reward\". This takes place in a perception-action-learning loop, whereby the agent receives a state/observation at every timestep, uses its policy to choose an action conditional on this observation, and then receives another observation and a scalar reward as the environment transitions to the next timestep. Through multiple interactions with the environment, the goal of the agent is to improve its policy so that it can maximise its expected cumulative reward.\n",
    "\n",
    "![RL Loop](https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/action-perception.png)\n",
    "\n",
    "Unlike the usual supervised or unsupervised learning settings, the agent is responsible for the data it receives, and so it is far from \"independent and identically distributed\". Furthermore, there is a particularly difficult credit assignment problem - the agent has to work out which of its actions result in rewards, where the rewards caused by specific actions may appear a long time in the future. Nevertheless, RL provides a formal framework for studying this problem.\n",
    "\n",
    "Here we'll look at the advantage actor-critic (A2C) algorithm, which combines an explicit policy (the actor) with value function estimation (the critic); the value function captures the expected cumulative reward from a given state and given a certain policy, or more informally how good a state is.\n",
    "\n",
    "We'll look at learning to solve two classic control problems - cartpole and pendulum - using A2C. In contrast to the field of optimal control, we're not assuming that we have access to a (dynamics/forward) model of the environment. RL also includes the concept of model-based methods, which attempt to learn and use such models from data, but that won't be covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "Firstly we'll instantiate the two environments and see how a random policy performs over several rollouts. This'll give us an idea of the problem to solve, as well as how a naive baseline would perform on it. These are both episodic environments, which means that they can terminate at some point. With cartpole, a random policy fails to keep the pole balanced and episodes terminate quickly. With pendulum, a random policy is unlikely to swing the pole up into the upright position, so episodes time out with low rewards.\n",
    "\n",
    "Just to note - we'll look at renders of the environment, but the observations for the agents will be a few symbolic inputs, such as the angle of the pole in the cartpole environment. However, the ability to learn policies \"end-to-end\", i.e., directly from pixels, is one of the major success of deep reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from IPython.display import clear_output, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Average reward: 22.20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABHhJREFUeJzt29FNG0EUQFFv5CZSRygjdUAbtAF1pIykjpSx+bGEZQysCbDeuedISICE9T7M1WrmeZrneQfA2L6tPQAAn0/sAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBgv/YABz7GC/Dc9FEv5MkeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiBA7AEC9msPAGv583j37Hc/bh9WmAQ+nyd7gACxBwgQe4AAsQcIEHs4cu7SFkYg9gABYk+WNUtKxB4gQOzhhHN7RiT2AAFiDxAg9gABYk+ajRwqxB4gQOzhDBs5jEbsAQLEnjzn9hSIPUCA2AMEiD1AgNjDC2zkMBKxh51LWsYn9gABYg8QIPbwCuf2jELsAQLEHiBA7OHARg4jE3uAALGHN7ikZQRiDxAg9gABYg8QIPZwxEYOoxJ7gACxhwVs5LB1Yg8QIPZwwrk9IxJ7gACxh4Wc27NlYg8QIPYAAWIPZ7ikZTRiDxAg9gABYg8XsJHDVok9QIDYAwSIPbzARg4jEXuAALGHC7mkZYvEHiBA7OEVzu0ZhdgDBIg9QIDYwzu4pGVrxB4gQOwBAsQe3mAjhxGIPWnTNC36+p+/fe014KuIPbzT74fbtUeAxfZrDwBb8uvvU+B/fn9ccRK4jCd7WOg49Od+hmsm9gABYg8L3Nw5smHbxB4WOj2jd2bPlrighQvc3988fb/eGHCxaZ7ntWfY7Xa7qxiCnq/cf7+S/zW25cPeoI5xAALEHiBA7AECxB4gQOwBAsQeIEDsAQLEHiDAJ2hJ80EnKjzZAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AMEiD1AgNgDBIg9QIDYAwSIPUCA2AME7Nce4GBaewCAkXmyBwgQe4AAsQcIEHuAALEHCBB7gACxBwgQe4AAsQcIEHuAALEHCBB7gACxBwgQe4AAsQcIEHuAALEHCBB7gACxBwgQe4AAsQcIEHuAALEHCPgH0LNB8b3Z1WoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis('off')\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()\n",
    "rollouts = 5\n",
    "total_reward = 0\n",
    "view = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "for _ in range(rollouts):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        view.set_data(env.render(mode='rgb_array'))\n",
    "        display(plt.gcf())\n",
    "        clear_output(wait=True)\n",
    "        _, reward, done, _ = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "\n",
    "env.close()\n",
    "display('Average reward: %.2f' % (total_reward / rollouts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Average reward: -1405.25'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAD8CAYAAAC2EFsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABA9JREFUeJzt2sFtGlEUQNEZi10qcB12qjCuI6WkhtAGdOFk5SK8t5SdpckqEUYJjuCaQeacFQhp9CTQ5Q2fcZqmAaBwNfcAwMchKEBGUICMoAAZQQEyggJkBAXICAqQERQgs5h7gDf4Gy+8v7G6kA0FyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKLuQfgY/hxf//q+bfr62G1Ws00DXOxoXC03ZgMwzB8eXoarq58vC6Nd5yj/C0mvz3c3Q3Pz88nnIa5jdM0zT3DPmc93KXbjsntZvPqte/L5Z/HN+v1yWbiIGN1IRsKB9uNCAgKkBEUDrZ9WwPD4NiYI33ebIaH5fKfcfn58nLiiZiTDYWjPOzZUm43m+HTwnfWJREUErs/0H59fHRLdIEcG3Owff9B2ebY+Ow5NgbOj6AAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAoHu1mv5x6BMyMovCvRuSyCwlH2BUNMLs84TdPcM+xz1sPBBzFWF7KhABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZAQFyAgKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCCzmHuAN4xzDwD8PxsKkBEUICMoQEZQgIygABlBATKCAmQEBcgICpARFCAjKEBGUICMoAAZQQEyggJkBAXICAqQERQgIyhARlCAjKAAGUEBMoICZH4BKH1BJoKbeGUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.axis('off')\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "view = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "for _ in range(rollouts):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        view.set_data(env.render(mode='rgb_array'))\n",
    "        display(plt.gcf())\n",
    "        clear_output(wait=True)\n",
    "        _, reward, done, _ = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "\n",
    "env.close()\n",
    "display('Average reward: %.2f' % (total_reward / rollouts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We'll construct a small fully-connected network with one hidden layer for both environments. For discrete control in cartpole (left or right) we'll use a categorical policy, and for continuous control in pendulum (torque) we'll use a Gaussian policy. For the latter we can actually sample using the reparameterisation trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    def __init__(self, obs_size, env):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.fc1 = nn.Linear(obs_size, 32)\n",
    "        if env == 'cartpole':\n",
    "            self.fc2 = nn.Linear(32, 1 + 2)\n",
    "        elif env == 'pendulum':\n",
    "            self.fc2 = nn.Linear(32, 1 + 1)\n",
    "            self.policy_std_dev = nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.fc2(torch.tanh(self.fc1(obs)))\n",
    "        value, policy = x[:, 0], x[:, 1:]\n",
    "        if self.env == 'cartpole':\n",
    "            policy = F.softmax(policy, dim=1)\n",
    "            action = policy.multinomial(1).squeeze(1)\n",
    "        elif self.env == 'pendulum':\n",
    "            policy = (policy, F.softplus(self.policy_std_dev))\n",
    "            action = policy[0] + policy[1] * torch.randn_like(policy[0])\n",
    "        return action, policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The first part of A2C training involves taking several rollouts in the environment. The environment is reset to start a new episode, and the agent takes actions sampled from its policy until the episode terminates. Meanwhile, we'll need to keep track of the policy, value estimates and rewards encountered at every timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 15.75'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 31.16'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 47.69'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 66.53'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 82.97'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 102.03'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 119.56'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 135.75'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 150.72'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 168.47'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 184.44'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 200.81'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 219.31'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 235.22'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 251.44'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 268.47'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 283.59'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 300.66'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 317.44'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average reward: 333.12'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = A2C(4, 'cartpole')\n",
    "optimiser = optim.RMSprop(agent.parameters(), lr=1e-3)\n",
    "value_loss_weight = 0.5\n",
    "entropy_loss_weight = 0.01\n",
    "discount = 0.99\n",
    "epochs = 20\n",
    "rollouts = 32\n",
    "total_reward = 0\n",
    "\n",
    "\n",
    "def obs_to_tensor(obs):\n",
    "    return torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "optimiser.zero_grad()\n",
    "for _ in range(epochs):\n",
    "    for _ in range(rollouts):\n",
    "        obs, done = obs_to_tensor(env.reset()), False\n",
    "        values, log_probs, rewards, entropies = [], [], [], []\n",
    "        while not done:\n",
    "            action, policy, value = agent(obs)\n",
    "            obs, reward, done, _ = env.step(action.item())\n",
    "            obs = obs_to_tensor(obs)\n",
    "            total_reward += reward\n",
    "\n",
    "            log_policy = policy.log()\n",
    "            entropy = -(log_policy * policy).sum(1)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_policy.gather(1, action.unsqueeze(1)))\n",
    "            entropies.append(entropy)\n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        trajectory_length = len(rewards)\n",
    "        loss = 0\n",
    "        for i in reversed(range(trajectory_length)):\n",
    "            R = rewards[i] + discount * R\n",
    "            A = R - values[i]\n",
    "            loss += value_loss_weight * A ** 2\n",
    "            loss -= log_probs[i] * A.detach()\n",
    "            loss -= entropy_loss_weight * entropies[i]\n",
    "        loss.backward()  \n",
    "    display('Average reward: %.2f' % (total_reward / rollouts))\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b23048805897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mlog_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_policy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'log'"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = A2C(3, 'pendulum')\n",
    "optimiser = optim.RMSprop(agent.parameters(), lr=1e-3)\n",
    "total_reward = 0\n",
    "\n",
    "optimiser.zero_grad()\n",
    "for _ in range(epochs):\n",
    "    for _ in range(rollouts):\n",
    "        obs, done = obs_to_tensor(env.reset()), False\n",
    "        values, log_probs, rewards, entropies = [], [], [], []\n",
    "        while not done:\n",
    "            action, policy, value = agent(obs)\n",
    "            obs, reward, done, _ = env.step([action.item()])\n",
    "            obs = obs_to_tensor(obs)\n",
    "            total_reward += reward\n",
    "\n",
    "            log_policy = policy.log()\n",
    "            entropy = -(log_policy * policy).sum(1)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_policy.gather(1, action.unsqueeze(1)))\n",
    "            entropies.append(entropy)\n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        trajectory_length = len(rewards)\n",
    "        loss = 0\n",
    "        for i in reversed(range(trajectory_length)):\n",
    "            R = rewards[i] + discount * R\n",
    "            A = R - values[i]\n",
    "            loss += value_loss_weight * A ** 2\n",
    "            loss -= log_probs[i] * A.detach()\n",
    "            loss -= entropy_loss_weight * entropies[i]\n",
    "        loss.backward()  \n",
    "    display('Average reward: %.2f' % (total_reward / rollouts))\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
